#' Collection of Information-Based Feature Selection Filters
#' 
#' Praznik is a collection of feature selection filters performing greedy optimisation of mutual information-based usefulness criteria.
#'
#' In a nutshell, an algorithm of this class requires an information system \eqn{(X,Y)} and a predefined number of features to selected \eqn{k}, and works like this.
#' To start, it estimates mutual information between each feature and the decision, find a feature with maximal such score and stores it as a first on a list of selected features, \eqn{S}.
#' Then, it estimates a value of a certain criterion function \eqn{J(X,Y,S)} for each feature \eqn{X}; this function also depends on \eqn{Y} and the set of already selected features \eqn{S}.
#' As in the first step, the previously unselected feature with a greatest value of the criterion function is selected next.
#' This is repeated until the method would gather \eqn{k} features, or, in case of some methods, when no more informative features can be found.
#' The methods implemented in praznik consider the following criteria.
#'
#' The mutual information maximisation filter, \code{\link{MIM}}, simply selects top-\eqn{k} features of best mutual information, that is
#' \deqn{J_{MIM}=I(X;Y).}
#'
#' The minimal conditional mutual information maximisation proposed by F. Fleauret, \code{\link{CMIM}}, uses
#' \deqn{J_{CMIM}(X)=\min_{W\in S} I(X;Y|W);}
#' this method is also effectively identical to the information fragments method.
#' 
#' The minimum redundancy maximal relevancy proposed by H. Peng et al., \code{\link{MRMR}}, uses
#' \deqn{J_{MRMR}=I(X;Y)-\frac{1}{|S|}\sum_{W\in S} I(X;W).}
#'
#' The joint mutual information filter by H. Yang and J. Moody, \code{\link{JMI}}, uses
#' \deqn{J_{JMI}=\sum_{W\in S} I(X,W;Y).}
#'
#' The double input symmetrical relevance filter by P. Meyer and G. Bontempi, \code{\link{DISR}}, uses
#' \deqn{J_{DISR}(X)=\sum_{W\in S} \frac{I(X,W;Y)}{H(X,W,Y)}.}
#'
#' The minimal joint mutual information maximisation filter by M. Bennasar, Y. Hicks and R. Setchi, \code{\link{JMIM}}, uses
#' \deqn{J_{JMIM}=\min_{W\in S} I(X,W;Y).}
#'
#' The minimal normalised joint mutual information maximisation filter by the same authors, \code{\link{NJMIM}}, uses
#' \deqn{J_{NJMIM}=\min_{W\in S} \frac{I(X,W;Y)}{H(X,W,Y)}.}
#'
#' While \code{\link{CMIM}}, \code{\link{JMIM}} and \code{\link{NJMIM}} consider minimal value over already selected features, they may use a somewhat more sophisticated and faster algorithm.
#'
#' The package also provides functions for scoring features.
#' 
#' \code{\link{miScores}} returns 
#' \deqn{I(X;Y).}
#'
#' \code{\link{cmiScores}} returns, for a given condition vector \eqn{Z},
#' \deqn{I(X;Y|Z).}
#'
#' \code{\link{jmiScores}} returns
#' \deqn{I(X,Z;Y).}
#'
#' \code{\link{njmiScores}} returns
#' \deqn{\frac{I(X,Z;Y)}{H(X,Y,Z)}.}
#'
#' Estimation of mutual information and its generalisations is a hard task; still, praznik aims at speed and simplicity and hence only offers basic, maximum likelihood estimator applicable on discrete data.
#' For convenience, praznik automatically and silently coerces non-factor inputs into about ten equally-spaced bins, following the heuristic often used in literature.
#'
#' Additionally, praznik has a limited, experimental support for replacing entropic statistics with Gini impurity-based; in such framework, entropy is replaced by Gini impurity
#' \deqn{g(X):=1-\sum_x p_x^2,}
#' which leads to an impurity gain
#' \deqn{G(X;Y):=g(Y)-E(g(Y)|X)=\sum_{xy}\frac{p_{xy}^2}{p_x}-\sum_{y} p_y^2,}
#' a counterpart of mutual information or information gain.
#' It does not possess most of elegant properties of mutual information, yet values of both are usually highly correlated; moreover, Gini gain is computationally easier to calculate, hence it often replaces MI in performance-sensitive applications, like optimising splits in decision trees.
#'
#' In a present version, praznik includes \code{\link{impScores}} for generating values of \eqn{G} for all features (an analog of \code{\link{miScores}}, as well as \code{\link{JIM}}, a Gini gain-based feature selection method otherwise identical to \code{\link{JMI}}.
#' @references "Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection" G. Brown et al. JMLR (2012).
"_PACKAGE"


# Load the native code

#' @useDynLib praznik, .registration=TRUE
NULL

# Some example data

#' Pre-discretised Madelon dataset
#'
#' Madelon is a synthetic data set from the NIPS 2003 feature selection challenge, generated by Isabelle Guyon.
#' It contains 480 irrelevant and 20 relevant features, including 5 informative and 15 redundant.
#' In this version, the originally numerical attributes have been pre-discetised into 10 bins, as well as their names have been altered to reveal 20 relevant features (as identified by the Boruta method).
#' @format A list with two elements, \code{X} containing a data frame with predictors, and \code{Y}, the decision. 
#' Features are in the same order as in the original data; the names of relevant ones start with \code{Rel}, while of irrelevant ones with \code{Irr}.
#' @source \url{https://archive.ics.uci.edu/ml/datasets/Madelon}
"MadelonD"

# Utilities

#' Control OpenMP thread count
#'
#' Does nothing, left to provide warnings to an old code.
#' @param threads Ignored.
#' @return Invisible \code{NULL}.
#' @note Since praznik 4.0, please use \code{threads} argument of particular algorithm function to control how many threads it will use.
#' @export
setOmpThreads<-function(threads){
 warning("setOmpThreads() is deprecated and does nothing; please use the 'threads' argument!")
 invisible(NULL)
}
 
